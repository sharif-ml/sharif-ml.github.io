{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNCp2mpVkqX9"
      },
      "source": [
        "# Mount Google Drive\n",
        "\n",
        "Due to convenience and fast downloading, I upload the raw data and fast-text .bin to google drive. Mounting these resources are as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zbRfE3E4Wvb",
        "outputId": "c427b28a-36df-4a1a-e6a5-5558f6b627ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epkx4xR2mAcs"
      },
      "source": [
        "# Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrgQyY-H4vY5",
        "outputId": "3f2dfb66-24f2-41a8-9cc4-015e54140f62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dadmatools\n",
            "  Downloading dadmatools-1.5.2-py3-none-any.whl (862 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.6/862.6 KB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.8/dist-packages (5.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.7/316.7 KB\u001b[0m \u001b[31m329.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wordcloud_fa\n",
            "  Downloading wordcloud_fa-0.1.10-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 KB\u001b[0m \u001b[31m289.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.9-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m332.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m363.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py7zr>=0.17.2\n",
            "  Downloading py7zr-0.20.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 KB\u001b[0m \u001b[31m260.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting conllu\n",
            "  Downloading conllu-4.5.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting transformers>=4.9.1\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m272.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sklearn>=0.0\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 KB\u001b[0m \u001b[31m322.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting NERDA\n",
            "  Downloading NERDA-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting pytorch-transformers>=1.1.0\n",
            "  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 KB\u001b[0m \u001b[31m324.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperopt>=0.2.5\n",
            "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m330.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate>=0.8.6 in /usr/local/lib/python3.8/dist-packages (from dadmatools) (0.8.10)\n",
            "Collecting supar==1.1.2\n",
            "  Downloading supar-1.1.2-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 KB\u001b[0m \u001b[31m301.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Deprecated==1.2.6\n",
            "  Downloading Deprecated-1.2.6-py2.py3-none-any.whl (8.1 kB)\n",
            "Requirement already satisfied: folium>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from dadmatools) (0.12.1.post1)\n",
            "Requirement already satisfied: gdown>=4.3.1 in /usr/local/lib/python3.8/dist-packages (from dadmatools) (4.4.0)\n",
            "Requirement already satisfied: spacy>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from dadmatools) (3.4.4)\n",
            "Collecting pyconll>=3.1.0\n",
            "  Downloading pyconll-3.1.0-py3-none-any.whl (26 kB)\n",
            "Collecting bpemb>=0.3.3\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Collecting h5py>=3.3.0\n",
            "  Downloading h5py-3.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: gensim>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from dadmatools) (3.6.0)\n",
            "Collecting html2text\n",
            "  Downloading html2text-2020.1.16-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from dadmatools) (1.13.1+cu116)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from Deprecated==1.2.6->dadmatools) (1.14.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from supar==1.1.2->dadmatools) (0.3.6)\n",
            "Collecting stanza\n",
            "  Downloading stanza-1.4.2-py3-none-any.whl (691 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m691.3/691.3 KB\u001b[0m \u001b[31m379.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from plotly) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly) (8.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Collecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 KB\u001b[0m \u001b[31m341.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m358.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pillow>=7.0.0 in /usr/local/lib/python3.8/dist-packages (from wordcloud_fa) (7.1.2)\n",
            "Requirement already satisfied: wordcloud==1.8.2.2 in /usr/local/lib/python3.8/dist-packages (from wordcloud_fa) (1.8.2.2)\n",
            "Collecting arabic-reshaper>=2.1.3\n",
            "  Downloading arabic_reshaper-3.0.0-py3-none-any.whl (20 kB)\n",
            "Collecting python-bidi==0.4.2\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from wandb) (4.4.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.14.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 KB\u001b[0m \u001b[31m297.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.25.1)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m338.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.30.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (23.0)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from bpemb>=0.3.3->dadmatools) (4.64.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m123.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.9 in /usr/local/lib/python3.8/dist-packages (from folium>=0.2.1->dadmatools) (2.11.3)\n",
            "Requirement already satisfied: branca>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from folium>=0.2.1->dadmatools) (0.6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown>=4.3.1->dadmatools) (4.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown>=4.3.1->dadmatools) (3.9.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.6.0->dadmatools) (6.3.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m202.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.5->dadmatools) (3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.5->dadmatools) (0.16.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from hyperopt>=0.2.5->dadmatools) (2.2.1)\n",
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 KB\u001b[0m \u001b[31m338.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex>=3.6.6\n",
            "  Downloading pycryptodomex-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m351.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzstd>=0.14.4\n",
            "  Downloading pyzstd-0.15.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.0/379.0 KB\u001b[0m \u001b[31m319.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting brotli>=1.0.9\n",
            "  Downloading Brotli-1.0.9-cp38-cp38-manylinux1_x86_64.whl (357 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 KB\u001b[0m \u001b[31m339.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybcj>=0.6.0\n",
            "  Downloading pybcj-1.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 KB\u001b[0m \u001b[31m238.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multivolumefile>=0.2.3\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting inflate64>=0.3.1\n",
            "  Downloading inflate64-0.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.5/94.5 KB\u001b[0m \u001b[31m247.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting texttable\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pyppmd<1.1.0,>=0.18.1\n",
            "  Downloading pyppmd-1.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.7/139.7 KB\u001b[0m \u001b[31m302.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from pytorch-transformers>=1.1.0->dadmatools) (2022.6.2)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.64-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 KB\u001b[0m \u001b[31m173.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m355.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m294.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (3.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (0.7.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (3.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (1.10.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (1.0.9)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (0.10.1)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (1.0.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (3.0.12)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (2.4.5)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (8.1.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy>=3.0.0->dadmatools) (2.0.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m333.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m269.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting progressbar\n",
            "  Downloading progressbar-2.5.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2>=2.9->folium>=0.2.1->dadmatools) (2.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (6.0.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0.0->dadmatools) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0.0->dadmatools) (0.7.9)\n",
            "Collecting botocore<1.30.0,>=1.29.64\n",
            "  Downloading botocore-1.29.64-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m185.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 KB\u001b[0m \u001b[31m267.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.7.1)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m338.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.12.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "Building wheels for collected packages: nltk, libwapiti, sklearn, pathtools, progressbar, sacremoses, emoji\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394486 sha256=79082a35ec69efa516caaca28fd80010ecd232885db744ca3421dbcdb7f73cf7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ssb2k5gt/wheels/19/1d/3a/0a8c14c30132b4f9ffd796efbb6746f15b3d6bcfc1055a9346\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp38-cp38-linux_x86_64.whl size=180761 sha256=bc6a05b8e886d11f6f3b459252c9257afa61bfe0a6e69bd2f294630a4838a572\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ssb2k5gt/wheels/3c/d8/9f/59fd78b2b7d1e9ffcb68fb6de80c2e7c20b804c8cbc4d8fc23\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=48edaa9374bbdc81e9463442590ebbe39f095716dd26fb7d2b42bd2a9395afae\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ssb2k5gt/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=29e0198f1478a1137d2a565a66bd177fad81f66616ee5d3c615b460cbc0b74ac\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ssb2k5gt/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12081 sha256=0afca771eb34510bc2c71fe477f73dcf794ef5ab3d5dcdc18271aabfe09c67c1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ssb2k5gt/wheels/2c/67/ed/d84123843c937d7e7f5ba88a270d11036473144143355e2747\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=0046ecf46aa568165b1a69e4ebdf76dfc21decd6d95e86d9f9a08f072f1fa11e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ssb2k5gt/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=6c9f6f14887825b37bab30b7ac2c537eb772735f716164223e96ba87e4c0e314\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ssb2k5gt/wheels/86/62/9e/a6b27a681abcde69970dbc0326ff51955f3beac72f15696984\n",
            "Successfully built nltk libwapiti sklearn pathtools progressbar sacremoses emoji\n",
            "Installing collected packages: tokenizers, tf-estimator-nightly, texttable, sklearn, sentencepiece, py4j, progressbar, pathtools, brotli, arabic-reshaper, urllib3, tensorflow-addons, smmap, setproctitle, segtok, sacremoses, pyzstd, python-bidi, pyppmd, pycryptodomex, pyconll, pybcj, nltk, multivolumefile, libwapiti, jmespath, inflate64, html2text, h5py, emoji, docker-pycreds, Deprecated, conllu, sentry-sdk, py7zr, hyperopt, hazm, gitdb, botocore, stanza, s3transfer, huggingface-hub, GitPython, bpemb, wordcloud_fa, wandb, transformers, boto3, supar, pytorch-transformers, NERDA, dadmatools\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: hyperopt\n",
            "    Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "Successfully installed Deprecated-1.2.6 GitPython-3.1.30 NERDA-1.0.0 arabic-reshaper-3.0.0 boto3-1.26.64 botocore-1.29.64 bpemb-0.3.4 brotli-1.0.9 conllu-4.5.2 dadmatools-1.5.2 docker-pycreds-0.4.0 emoji-2.2.0 gitdb-4.0.10 h5py-3.8.0 hazm-0.7.0 html2text-2020.1.16 huggingface-hub-0.12.0 hyperopt-0.2.7 inflate64-0.3.1 jmespath-1.0.1 libwapiti-0.2.1 multivolumefile-0.2.3 nltk-3.3 pathtools-0.1.2 progressbar-2.5 py4j-0.10.9.7 py7zr-0.20.2 pybcj-1.0.1 pyconll-3.1.0 pycryptodomex-3.17 pyppmd-1.0.0 python-bidi-0.4.2 pytorch-transformers-1.2.0 pyzstd-0.15.3 s3transfer-0.6.0 sacremoses-0.0.53 segtok-1.5.11 sentencepiece-0.1.97 sentry-sdk-1.14.0 setproctitle-1.3.2 sklearn-0.0.post1 smmap-5.0.0 stanza-1.4.2 supar-1.1.2 tensorflow-addons-0.19.0 texttable-1.6.7 tf-estimator-nightly-2.8.0.dev2021122109 tokenizers-0.13.2 transformers-4.26.0 urllib3-1.26.14 wandb-0.13.9 wordcloud_fa-0.1.10\n"
          ]
        }
      ],
      "source": [
        "!pip3 --no-cache-dir install dadmatools numpy pandas matplotlib plotly scikit-learn hazm wordcloud_fa nltk wandb tensorflow tensorflow-addons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0PSq0Cmpoh1"
      },
      "source": [
        "# Import Required Functionalities \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HHj-j8rVpsWF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "from hazm import Normalizer, sent_tokenize, word_tokenize, Stemmer, Lemmatizer, POSTagger, Chunker, tree2brackets, DependencyParser, stopwords_list\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dropout\n",
        "\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmGydjI0mS9J"
      },
      "source": [
        "# Read Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "IOujvkW94Kny",
        "outputId": "9968ce83-3f1f-4799-a78e-62ae5856d97c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b26bee7c-c45b-4f6a-9c4a-66d164d0fdfd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>131244574</td>\n",
              "      <td>عالی عالی عالی عالی عالی عالی عالی عالی عالی ع...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>133141894</td>\n",
              "      <td>دوستان این نظرات و پیشنهادات رو باید به پشتیبا...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>94129372</td>\n",
              "      <td>خیلی ایراد داره مسخره تر از این نمیشه رقیب‌هات...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>131334220</td>\n",
              "      <td>نه کی گفته خرابه من دارم باهاش کار میکنم از من...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>131387172</td>\n",
              "      <td>سلام عالیه حتما نصب کنید از کالاف دیوتی هم بهتره</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3591</th>\n",
              "      <td>94229465</td>\n",
              "      <td>همه رشته ها نداره مثلا معارف سوالات تخصصي ندار...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3592</th>\n",
              "      <td>131571104</td>\n",
              "      <td>خیلی بده من اصلا. دوست ندارم خواهش می کنم دانل...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3593</th>\n",
              "      <td>132784715</td>\n",
              "      <td>بهترین برنا مه ای که دیدم خیلی باهاله میتونی ت...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3594</th>\n",
              "      <td>131981378</td>\n",
              "      <td>خیلی بازی مسخره هس نصب نکنید ۱ستاره هم براش زیاده</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3595</th>\n",
              "      <td>132039277</td>\n",
              "      <td>خیلی بده به نارنجی میگه نارَ نارنجی</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3596 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b26bee7c-c45b-4f6a-9c4a-66d164d0fdfd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b26bee7c-c45b-4f6a-9c4a-66d164d0fdfd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b26bee7c-c45b-4f6a-9c4a-66d164d0fdfd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "             id                                               text  label\n",
              "0     131244574  عالی عالی عالی عالی عالی عالی عالی عالی عالی ع...      0\n",
              "1     133141894  دوستان این نظرات و پیشنهادات رو باید به پشتیبا...      0\n",
              "2      94129372  خیلی ایراد داره مسخره تر از این نمیشه رقیب‌هات...      1\n",
              "3     131334220  نه کی گفته خرابه من دارم باهاش کار میکنم از من...      0\n",
              "4     131387172   سلام عالیه حتما نصب کنید از کالاف دیوتی هم بهتره      0\n",
              "...         ...                                                ...    ...\n",
              "3591   94229465  همه رشته ها نداره مثلا معارف سوالات تخصصي ندار...      1\n",
              "3592  131571104  خیلی بده من اصلا. دوست ندارم خواهش می کنم دانل...      0\n",
              "3593  132784715  بهترین برنا مه ای که دیدم خیلی باهاله میتونی ت...      0\n",
              "3594  131981378  خیلی بازی مسخره هس نصب نکنید ۱ستاره هم براش زیاده      0\n",
              "3595  132039277                خیلی بده به نارنجی میگه نارَ نارنجی      0\n",
              "\n",
              "[3596 rows x 3 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "labelled_data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SAMousavizade/data_labelled.csv\")\n",
        "unlabelled_data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/SAMousavizade/data_unlabelled.csv\")\n",
        "\n",
        "labelled_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZSQXKiCmVOC"
      },
      "source": [
        "# Preprocess Raw Data\n",
        "\n",
        "In this section, I preprocess raw-text data. Text preprocessing steps are as follows:\n",
        "\n",
        "- Unifying all variants of characters (like \"ی\" and \"ي\")\n",
        "- Remove extra spaces between tokens \n",
        "- Remove punctuations(like !, ., ?, etc)\n",
        "- Remove HTML tags  \n",
        "- Remove all emails, phone numbers, URLs, emojis\n",
        "- Remove stop-words \n",
        "-  Refine any characters being repeated more than 2 times in the tokens (like \"عااااااااالیه\" to \"عاالیه\"\n",
        "- Lemmatization (grouping the inflected forms of a word so they can be analyzed as a single item.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on76gUASpcCH"
      },
      "source": [
        "apply this steps on training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YIDkUbR24Knz"
      },
      "outputs": [],
      "source": [
        "from dadmatools.models.normalizer import Normalizer\n",
        "\n",
        "normalizer = Normalizer(\n",
        "    full_cleaning=False,\n",
        "    unify_chars=True,\n",
        "    refine_punc_spacing=True,\n",
        "    remove_extra_space=True,\n",
        "    remove_puncs=True,\n",
        "    remove_html=True,\n",
        "    remove_stop_word=True,\n",
        "    replace_email_with=\"\",\n",
        "    replace_number_with=\"\",\n",
        "    replace_url_with=\"\",\n",
        "    replace_mobile_number_with=\"\",\n",
        "    replace_emoji_with=\"\",\n",
        "    replace_home_number_with=\"\"\n",
        ")\n",
        "\n",
        "labelled_data[\"normalized_text\"] = labelled_data[\"text\"].apply(lambda text: normalizer.normalize(text))\n",
        "labelled_data[\"normalized_text\"] = labelled_data[\"normalized_text\"].replace(r'[^آ-یA-Za-z0-9 ]+', '', regex=True)\n",
        "labelled_data[\"normalized_text\"] = labelled_data[\"normalized_text\"].replace(r'(.)\\1{2,}', '', regex=True)\n",
        "\n",
        "unlabelled_data[\"normalized_text\"] = unlabelled_data[\"text\"].apply(lambda text: normalizer.normalize(text))\n",
        "unlabelled_data[\"normalized_text\"] = unlabelled_data[\"normalized_text\"].replace(r'[^آ-یA-Za-z0-9 ]+', '', regex=True)\n",
        "unlabelled_data[\"normalized_text\"] = unlabelled_data[\"normalized_text\"].replace(r'(.)\\1{2,}', '', regex=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV-x9a9zpUtF"
      },
      "source": [
        "apply previously discussed preprocessing steps on test data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "q6nUFOSKK2xo",
        "outputId": "3ee71815-6092-429c-92f5-3f15cac73763"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-43f4860f-0739-444a-84be-78a4555ffc80\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>normalized_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>!!!!سلام برنامه خوبیه جدا</td>\n",
              "      <td>سلام برنامه خوبیه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>لود نمیشه اصلا!! :((((</td>\n",
              "      <td>لود نمیشه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>پولم رو پس نمیدید چرا؟؟؟</td>\n",
              "      <td>پولم نمیدید</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>بازی جالبیه.</td>\n",
              "      <td>بازی جالبیه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>خیلییییی لگ داره روی گوشیم.</td>\n",
              "      <td>خیل لگ داره گوشیم</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>معتاد این بازی شدم.</td>\n",
              "      <td>معتاد بازی</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>خیلی باگ داره اعصابو خورد کرده.</td>\n",
              "      <td>باگ داره اعصابو خورد</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>بازی توی مرحله اول گیر کرده و به مرحله بعدی نم...</td>\n",
              "      <td>بازی مرحله مرحله بعدی نمیره</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>آقا عالیه!!!!</td>\n",
              "      <td>عالیه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>موقع نصب به مشکل میخوره. اه.</td>\n",
              "      <td>موقع نصب مشکل میخوره اه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>آشغااااااااااااااله</td>\n",
              "      <td>آشغله</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>افتضاحهههههه.</td>\n",
              "      <td>افتضاح</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>مزخرفه.</td>\n",
              "      <td>مزخرفه</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>همش باگ میخورههههههههههههههههههههه.</td>\n",
              "      <td>همش باگ میخور</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>برای بچه ها مشکل داره این بازی. لطفا اینو ذکر ...</td>\n",
              "      <td>بچه مشکل داره بازی ذکر</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-43f4860f-0739-444a-84be-78a4555ffc80')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-43f4860f-0739-444a-84be-78a4555ffc80 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-43f4860f-0739-444a-84be-78a4555ffc80');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                 text  \\\n",
              "0                           !!!!سلام برنامه خوبیه جدا   \n",
              "1                              لود نمیشه اصلا!! :((((   \n",
              "2                            پولم رو پس نمیدید چرا؟؟؟   \n",
              "3                                        بازی جالبیه.   \n",
              "4                         خیلییییی لگ داره روی گوشیم.   \n",
              "5                                 معتاد این بازی شدم.   \n",
              "6                     خیلی باگ داره اعصابو خورد کرده.   \n",
              "7   بازی توی مرحله اول گیر کرده و به مرحله بعدی نم...   \n",
              "8                                       آقا عالیه!!!!   \n",
              "9                        موقع نصب به مشکل میخوره. اه.   \n",
              "10                                آشغااااااااااااااله   \n",
              "11                                      افتضاحهههههه.   \n",
              "12                                            مزخرفه.   \n",
              "13                همش باگ میخورههههههههههههههههههههه.   \n",
              "14  برای بچه ها مشکل داره این بازی. لطفا اینو ذکر ...   \n",
              "\n",
              "                normalized_text  \n",
              "0             سلام برنامه خوبیه  \n",
              "1                     لود نمیشه  \n",
              "2                   پولم نمیدید  \n",
              "3                   بازی جالبیه  \n",
              "4             خیل لگ داره گوشیم  \n",
              "5                    معتاد بازی  \n",
              "6          باگ داره اعصابو خورد  \n",
              "7   بازی مرحله مرحله بعدی نمیره  \n",
              "8                         عالیه  \n",
              "9       موقع نصب مشکل میخوره اه  \n",
              "10                        آشغله  \n",
              "11                       افتضاح  \n",
              "12                       مزخرفه  \n",
              "13                همش باگ میخور  \n",
              "14       بچه مشکل داره بازی ذکر  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_case_data = pd.DataFrame({\"text\": [\n",
        "    \"!!!!سلام برنامه خوبیه جدا\",\n",
        "    \"لود نمیشه اصلا!! :((((\",\n",
        "    \"پولم رو پس نمیدید چرا؟؟؟\",\n",
        "    \"بازی جالبیه.\",\n",
        "    \"خیلییییی لگ داره روی گوشیم.\",\n",
        "    \"معتاد این بازی شدم.\",\n",
        "    \"خیلی باگ داره اعصابو خورد کرده.\",\n",
        "    \"بازی توی مرحله اول گیر کرده و به مرحله بعدی نمیره اصلا! :(((\",\n",
        "    \"آقا عالیه!!!!\",\n",
        "    \"موقع نصب به مشکل میخوره. اه.\",\n",
        "    \"آشغااااااااااااااله\",\n",
        "    \"افتضاحهههههه.\",\n",
        "    \"مزخرفه.\",\n",
        "    \"همش باگ میخورههههههههههههههههههههه.\",\n",
        "    \"برای بچه ها مشکل داره این بازی. لطفا اینو ذکر کنید.\"\n",
        "]})\n",
        "\n",
        "test_case_data[\"normalized_text\"] = test_case_data[\"text\"].apply(lambda text: normalizer.normalize(text))\n",
        "test_case_data[\"normalized_text\"] = test_case_data[\"normalized_text\"].replace(r'[^آ-یA-Za-z0-9 ]+', '', regex=True)\n",
        "test_case_data[\"normalized_text\"] = test_case_data[\"normalized_text\"].replace(r'(.)\\1{2,}', '', regex=True)\n",
        "\n",
        "test_case_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DueVxJ6lIxfS"
      },
      "outputs": [],
      "source": [
        "from hazm import Normalizer, sent_tokenize, word_tokenize, Stemmer, Lemmatizer, POSTagger, Chunker, tree2brackets, DependencyParser, stopwords_list\n",
        "\n",
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "labelled_data[\"lemmatized_tokens\"] = labelled_data[\"normalized_text\"].apply(lambda text: \" \".join(list(map(lemmatizer.lemmatize, word_tokenize(text)))))\n",
        "unlabelled_data[\"lemmatized_tokens\"] = unlabelled_data[\"normalized_text\"].apply(lambda text: \" \".join(list(map(lemmatizer.lemmatize, word_tokenize(text)))))\n",
        "test_case_data[\"lemmatized_tokens\"] = test_case_data[\"normalized_text\"].apply(lambda text: \" \".join(list(map(lemmatizer.lemmatize, word_tokenize(text)))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWw3IkwYvN80"
      },
      "source": [
        "# Create Tensorflow Dataset\n",
        "Create train, validation and test dataset tensorflow object from pre-processed texts and labels.\n",
        "\n",
        "The tf.data.Dataset API supports writing descriptive and efficient input pipelines. Dataset usage follows a common pattern: \n",
        "\n",
        "1. Create a source dataset from your input data.\n",
        "2. Apply dataset transformations to preprocess the data.\n",
        "3. Iterate over the dataset and process the elements.\n",
        "Iteration happens in a streaming fashion, so the full dataset does not need to fit into memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrHW-IZWnETI"
      },
      "source": [
        "# Define Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iv_iWKrUnGkf"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 3e-5\n",
        "INFORMATIVE_CLASS_WEIGHT = 5.0\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 25\n",
        "EMBEDDING_SIZE = 100\n",
        "\n",
        "BUFFER_SIZE = 512\n",
        "MAX_VOCAB_SIZE = 25000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBsdzOKXNDrN"
      },
      "source": [
        "## Split Data to Train and Validation\n",
        "\n",
        "Split preprocessed data to train and validation by the proportion of 0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mmpW4DFs4vaN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "  \n",
        "train, validation = train_test_split(\n",
        "    labelled_data,\n",
        "    random_state=104, \n",
        "    test_size=0.1, \n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrNnfgaHM7Tt"
      },
      "source": [
        "# Convert Data Labels to Dummy Variable\n",
        "\n",
        "To compare with predicted probabilities for each category that the model outputs in the softmax layer, I need to transform the label column to dummy variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OGCat-RCM_1B"
      },
      "outputs": [],
      "source": [
        "text_column_name = \"lemmatized_tokens\"\n",
        "label_column_name = \"label\"\n",
        "\n",
        "categories = [\"Non-Informative\", \"Informative\"]\n",
        "train_labels = pd.get_dummies(train[label_column_name])\n",
        "validation_labels = pd.get_dummies(validation[label_column_name])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7UqlvHTNRf9"
      },
      "source": [
        "## Transform Data To Tensorflow Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mYzAcIhAvLIn"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices( (train[text_column_name].tolist(), train_labels) ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices( (validation[text_column_name].values, validation_labels) ).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqd9wDS1veb_"
      },
      "source": [
        "# Train Pipeline\n",
        "\n",
        "## Use Pre-trained FastText Embedding Vectors\n",
        "download farsi word embedding vectors in format of `.bin` file from **[FastText Repository](https://fasttext.cc/docs/en/crawl-vectors.html)** and place in working directory. (for persian its 'cc.fa.300.bin'), alternatively mentioned .bin embedding vectors file can be downloaded with the following command (first change directory to directory that fasttext is installed):\n",
        "\n",
        "> ./download_model.py fa # farsi\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6xr6EK7yngd"
      },
      "source": [
        "# FastText Installation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJsECMlDys7b",
        "outputId": "67836ad5-0c2f-483c-a9df-cf4b3a310995"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 3930, done.\u001b[K\n",
            "remote: Counting objects: 100% (944/944), done.\u001b[K\n",
            "remote: Compressing objects: 100% (140/140), done.\u001b[K\n",
            "remote: Total 3930 (delta 854), reused 804 (delta 804), pack-reused 2986\u001b[K\n",
            "Receiving objects: 100% (3930/3930), 8.24 MiB | 22.33 MiB/s, done.\n",
            "Resolving deltas: 100% (2505/2505), done.\n",
            "/content/fastText\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/fastText\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.3-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from fasttext==0.9.2) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fasttext==0.9.2) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp38-cp38-linux_x86_64.whl size=4386484 sha256=c359de7dceebe1caf6b4d14ab4d050eb68060ea1c68c79119a2c5c99a27319f3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-newt8qyb/wheels/a4/2f/6a/91d479a807787e092f667baec5df08801b0558f5676427f5a9\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.3\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!rm -rf fastText\n",
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "%cd ./fastText/\n",
        "!sudo pip install .\n",
        "\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMfz9fPQ0GxP"
      },
      "source": [
        "Then verify the installation went well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dZGAdzTk0H-k"
      },
      "outputs": [],
      "source": [
        "import fasttext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5xWbIuiwEE9"
      },
      "source": [
        "Note: Jump to *Load Fine-Tuned FastText Language Model (Word Embeddings)* section if fine-tuned FastText language model is prepared previously.\n",
        "\n",
        "---\n",
        "\n",
        "# Download Original FastText Embeddings\n",
        "Download original fasttext embeddings in `.bin` file. its about 4.2Gb!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ONZwtFHIwRbx"
      },
      "outputs": [],
      "source": [
        "# from fasttext.util import reduce_model\n",
        "\n",
        "# fasttext.util.download_model('fa', if_exists='ignore')  # farsi"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1YLoy6o055r4"
      },
      "source": [
        "# Adapt The Dimension of Word Embeddings\n",
        "\n",
        "The pre-trained word vectors that distribute by FastText have a dimension of 300. I need a smaller size, So I use the dimension reducer, which is implemented in this package. This dimension reduction is done using the **PCA** algorithm. \n",
        "\n",
        "In this project, I reduce the dimension of word embeddings to 100 as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "M0QatxQcaLM5"
      },
      "outputs": [],
      "source": [
        "# import fasttext\n",
        "# from fasttext.util import reduce_model\n",
        "\n",
        "# ft = fasttext.load_model(f'/content/cc.fa.{300}.bin', )\n",
        "# print(\"model loaded ...\")\n",
        "\n",
        "# reduce_model(ft, EMBEDDING_SIZE)\n",
        "# print(\"embedding dimension reduced ...\")\n",
        "\n",
        "# ft.save_model(f\"/content/cc.fa.{EMBEDDING_SIZE}.bin\", )\n",
        "# print(\"reduced model saved ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMPHOsiL_3N2"
      },
      "source": [
        "### (Optional) Copy/Load Reduced Embeddings To/From Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yixVpBxC_25_"
      },
      "outputs": [],
      "source": [
        "########################################################################\n",
        "#### Embedding Size: 100\n",
        "########################################################################\n",
        "# !cp /content/cc.fa.100.bin /content/drive/MyDrive/FastText\n",
        "\n",
        "########################################################################\n",
        "#### Embedding Size: 300\n",
        "########################################################################\n",
        "# !cp /content/cc.fa.300.bin /content/drive/MyDrive/FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP9BntcoAWam"
      },
      "source": [
        "and load it from drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MP34Nb-pAVww"
      },
      "outputs": [],
      "source": [
        "# ft = fasttext.load_model(f'/content/drive/MyDrive/FastText/cc.fa.{EMBEDDING_SIZE}.bin', )\n",
        "# print(\"model loaded ...\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3rr_q1aMxQ3z"
      },
      "source": [
        "# Unsupervised Fine Tune Language Model (FastText Embeddings)\n",
        "\n",
        "In this section, I fine-tune FastText language model (word embeddings) unsupervised-ly using unlabelled data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waQ3u58O0Z2J"
      },
      "source": [
        "## Prepare (previously preprocessed) Unlabelled Data Texts\n",
        "\n",
        "Prepare unlabelled texts in `unlabelled_texts.txt`. (Each line contains one unlabelled text) that will be used in FastText embedding fine-tuning on this special context. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReBYvC1ssLQ1"
      },
      "source": [
        "Write unlabelled texts line by line in mentioned `.txt` file "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "PzNvF8Xqrspo"
      },
      "outputs": [],
      "source": [
        "# with open('/content/unlabelled_texts.txt', 'w') as file:\n",
        "#     unlabelled_data[\"lemmatized_tokens\"].apply(lambda text: file.write(f\"{text}\\n\"))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LPmjB2hsYXA"
      },
      "source": [
        "Generate .vec file from .bin file that is downloaded from FastText repo.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "f2fujmy14eSZ"
      },
      "outputs": [],
      "source": [
        "# def generate_vec_file_from_bin_file(fasttext_model, output_path_filename):\n",
        "    \n",
        "#     # get all words from model\n",
        "#     words = fasttext_model.get_words()\n",
        "\n",
        "#     with open(output_path_filename,'w') as file_out:\n",
        "        \n",
        "#         # the first line must contain number of total words and vector dimension\n",
        "#         file_out.write(str(len(words)) + \" \" + str(fasttext_model.get_dimension()) + \"\\n\")\n",
        "\n",
        "#         # line by line, you append vectors to VEC file\n",
        "#         for w in words:\n",
        "#             v = fasttext_model.get_word_vector(w)\n",
        "#             vstr = \"\"\n",
        "#             for vi in v:\n",
        "#                 vstr += \" \" + str(vi)\n",
        "\n",
        "#                 # to reduce .vec file volume\n",
        "#                 # vstr += \" \" + \"{:.4f}\".format(vi)\n",
        "#             try:\n",
        "#                 file_out.write(w + vstr+'\\n')\n",
        "#             except:\n",
        "#                 pass\n",
        "\n",
        "\n",
        "# generate_vec_file_from_bin_file(ft, output_path_filename=f'/content/cc.fa.{EMBEDDING_SIZE}.vec')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyE4qa91swC_"
      },
      "source": [
        "# Fine Tune FastText Language Model (Word Embeddings)\n",
        "\n",
        "Fine tune general FastText language model on current special contexts using unlabelled texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Fbznge0QojZw"
      },
      "outputs": [],
      "source": [
        "# ft = fasttext.train_unsupervised(\n",
        "#     input='/content/unlabelled_texts.txt',\n",
        "#     pretrainedVectors=f\"/content/drive/MyDrive/FastText/cc.fa.{EMBEDDING_SIZE}.vec\",\n",
        "#     dim=EMBEDDING_SIZE,\n",
        "#     verbose=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYCC0mN3tCSv"
      },
      "source": [
        "save fine-tuned language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-8kiJnVkqXZ7"
      },
      "outputs": [],
      "source": [
        "# ft.save_model(f\"cc.fa.{EMBEDDING_SIZE}_fine_tuned.bin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymOQ3x8JtGAc"
      },
      "source": [
        "copy to drive for nexts usages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pKYfhp3lrMoP"
      },
      "outputs": [],
      "source": [
        "# !cp /content/cc.fa.{EMBEDDING_SIZE}_fine_tuned.bin /content/drive/MyDrive/FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA7Md03ytTn-"
      },
      "source": [
        "--- "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxZuuuqetM1F"
      },
      "source": [
        "# Load Fine-Tuned FastText Language Model (Word Embeddings) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVayrDi8qggP",
        "outputId": "5cb1c104-24c9-4825-d5fb-ef8fac16d616"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model loaded ...\n"
          ]
        }
      ],
      "source": [
        "ft = fasttext.load_model(f'/content/drive/MyDrive/FastText/cc.fa.{EMBEDDING_SIZE}_fine_tuned.bin', )\n",
        "\n",
        "print(\"model loaded ...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uStZ3mU9wT3Z"
      },
      "source": [
        "# Create Embedding Matrix\n",
        "Create embedding matrix using pre-trained fasttext embedding that loaded previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "tFdYY44xwaYj"
      },
      "outputs": [],
      "source": [
        "vectorizer = tf.keras.layers.TextVectorization(max_tokens=MAX_VOCAB_SIZE)\n",
        "vectorizer.adapt(labelled_data[text_column_name].values)\n",
        "vocabulary = vectorizer.get_vocabulary()\n",
        "\n",
        "E = np.zeros((len(vocabulary), EMBEDDING_SIZE))\n",
        "for i, word in enumerate(vocabulary):\n",
        "    E[i] = ft.get_word_vector(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdgAxDdJwhr7"
      },
      "source": [
        "# Create Embedding Layer\n",
        "Create embedding layer with the help of embedding matrix (that created in previous step.) as initial state of this layer and set `trainable=True` to enhance embeddings during sequence classification supervised learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "nE5R8eRfwiWy"
      },
      "outputs": [],
      "source": [
        "from keras.initializers import Constant\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    len(vocabulary), EMBEDDING_SIZE,\n",
        "    embeddings_initializer=Constant(E),\n",
        "    trainable=True\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AKtCyqzpwyI1"
      },
      "source": [
        "# Define Model Architecture\n",
        "I use multi-layer bi-directional lstm layer in the model architecture because the text has no order and its not a generative task.\n",
        " \n",
        "**Bidirectional Long-Short Term Memory(LSTM)** is the process of making any neural network to have the sequence information in both directions backwards (future to past) or forward(past to future)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xplOURUfxM0s",
        "outputId": "2568d288-ba2b-483b-ec24-6d22a03b273a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, None)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 100)         689200    \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, None, 100)        160800    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 100)              160800    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 202       \n",
            "                                                                 \n",
            " softmax (Softmax)           (None, 2)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,021,102\n",
            "Trainable params: 1,021,102\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "n_class = len(categories)\n",
        "model = tf.keras.Sequential([\n",
        "    vectorizer,\n",
        "    embedding_layer,\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(EMBEDDING_SIZE, return_sequences=True), merge_mode=\"ave\"),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(EMBEDDING_SIZE), merge_mode=\"ave\"),\n",
        "    tf.keras.layers.Dense(EMBEDDING_SIZE, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(n_class),\n",
        "    tf.keras.layers.Softmax()\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kaQhXwQJxO_X"
      },
      "source": [
        "# Compile Model\n",
        "\n",
        "Because of **imbalanced weights of categories** in the label column, I use **ROC-AUC** (Area under the curve of ROC diagram) and **Precision & Recall** as metrics for model performance evaluation. \n",
        "\n",
        "Also due to the different importance of *Informative* category relative to *Non-Informative* in this context, I use the **Weighted Cross-Entropy Loss** trick described in the following section.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rmzce71ow38x"
      },
      "source": [
        "## Weighted Cross-Entropy Loss\n",
        "\n",
        "This is like traditional cross-entropy loss except that the weight term $w$ , allows one to trade off recall and precision by up- or down-weighting the cost of a positive error relative to a negative error.\n",
        "\n",
        "A value $w$ > 1 decreases the false negative count, hence increasing the recall. Conversely setting $w$ < 1 decreases the false positive count and increases the precision. This can be seen from the fact that $w$ is introduced as a multiplicative coefficient for the positive labels term in the loss expression:\n",
        "\n",
        "$$\n",
        "  −(w.y \\log(p)+(1−y)\\log(1−p))\n",
        "$$\n",
        "\n",
        "Due to the different importance of *Informative* category relative to *Non-Informative*, I set $w$ hyperparameter to 5. (`INFORMATIVE_CLASS_WEIGHT=5`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9fuku8-Rw3Of"
      },
      "outputs": [],
      "source": [
        "from keras.metrics import CategoricalAccuracy, AUC, Recall, RecallAtPrecision, Precision, PrecisionAtRecall, CategoricalCrossentropy\n",
        "from keras.losses import BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "import tensorflow_addons as tfa\n",
        "from keras.optimizers import Nadam\n",
        "from tensorflow.nn import weighted_cross_entropy_with_logits\n",
        "from keras import backend as K\n",
        "\n",
        "def WeightedCrossEntropy(labels, logits):\n",
        "    labels = tf.cast(labels, logits.dtype)\n",
        "    wce = weighted_cross_entropy_with_logits(labels[:, 1], logits[:, 1], INFORMATIVE_CLASS_WEIGHT, name=\"weighted_cross_entropy\")\n",
        "    return tf.reduce_mean(wce, axis=-1)  # Note the `axis=-1`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4gnIjYq0QSP"
      },
      "source": [
        "Compile model with the following:\n",
        "- Loss Criteria: Weighted Cross Entropy \n",
        "- Optimizer: Nadam \n",
        "- metrics: accuracy, roc-auc, recall and precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "I96CZEjCxuRt"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=WeightedCrossEntropy,\n",
        "              optimizer=Nadam(learning_rate=LEARNING_RATE),\n",
        "              metrics=[\n",
        "                  CategoricalAccuracy(name=\"accuracy\"),\n",
        "                  AUC(name=\"ROC-AUC\"),\n",
        "                  Recall(name=\"recall\", class_id=1),\n",
        "                  Precision(name=\"precision\", class_id=1),\n",
        "              ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItAN-Ky7lvE5"
      },
      "source": [
        "# Login to WandB "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jwv06Q6zly1t",
        "outputId": "7ac4d7ea-d546-4e70-9929-ca0d310017bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msamousavizade\u001b[0m (\u001b[33mcausal-inference\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"WANDB_API_KEY\"] = \"1d6bdaf3f9f088abf0915e5e5cb6689e4c7e7476\"\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leh00ewB0k84"
      },
      "source": [
        "# Initialize WandB Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "xQWzlmCjnaCk",
        "outputId": "699dfa5d-9cec-4070-8f5a-e199b08292a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msamousavizade\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230204_191749-dg1rnufx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/samousavizade/CoffeeBazaarSeqClassification/runs/dg1rnufx\" target=\"_blank\">bi-LSTM LR:3e-05 B:25 W:5.0 E:100</a></strong> to <a href=\"https://wandb.ai/samousavizade/CoffeeBazaarSeqClassification\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href=\"https://wandb.ai/samousavizade/CoffeeBazaarSeqClassification\" target=\"_blank\">https://wandb.ai/samousavizade/CoffeeBazaarSeqClassification</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href=\"https://wandb.ai/samousavizade/CoffeeBazaarSeqClassification/runs/dg1rnufx\" target=\"_blank\">https://wandb.ai/samousavizade/CoffeeBazaarSeqClassification/runs/dg1rnufx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "LEARNING_RATE = LEARNING_RATE\n",
        "INFORMATIVE_CLASS_WEIGHT = INFORMATIVE_CLASS_WEIGHT\n",
        "BATCH_SIZE = BATCH_SIZE\n",
        "EPOCHS = EPOCHS\n",
        "EMBEDDING_SIZE = EMBEDDING_SIZE\n",
        "\n",
        "BUFFER_SIZE = BUFFER_SIZE\n",
        "MAX_VOCAB_SIZE = MAX_VOCAB_SIZE \n",
        "\n",
        "wandb.init(\n",
        "  project=\"CoffeeBazaarSeqClassification \",\n",
        "  entity=\"samousavizade\",\n",
        "  name=f\"bi-LSTM LR:{LEARNING_RATE} B:{BATCH_SIZE} W:{INFORMATIVE_CLASS_WEIGHT} E:{EMBEDDING_SIZE}\",\n",
        "  config={\n",
        "      \"learning_rate\": LEARNING_RATE,\n",
        "      \"informative_class_weight\": INFORMATIVE_CLASS_WEIGHT,\n",
        "      \"batch_size\": BATCH_SIZE,\n",
        "      \"epochs\": EPOCHS,\n",
        "\n",
        "      \"embedding_size\": EMBEDDING_SIZE,\n",
        "      \"buffer_size\": BUFFER_SIZE,\n",
        "      \"max_vocab_size\": MAX_VOCAB_SIZE\n",
        "  })\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "\n",
        "metric_logger = WandbMetricsLogger(log_freq=\"epoch\")\n",
        "model_checkpoint = WandbModelCheckpoint(filepath=\"my_model_{epoch:02d}\", monitor=\"val_loss\", mode=\"min\", verbose=0, save_best_only=True, save_freq=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwIkoIGIx_bH"
      },
      "source": [
        "# Train and Validation Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCth83OVx_1f",
        "outputId": "25d2a6dd-ce27-48b7-b139-7c10f3e4cf92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "130/130 [==============================] - 16s 31ms/step - loss: 1.3926 - accuracy: 0.3520 - ROC-AUC: 0.3625 - recall: 0.9269 - precision: 0.3199 - val_loss: 1.3585 - val_accuracy: 0.3056 - val_ROC-AUC: 0.3808 - val_recall: 1.0000 - val_precision: 0.3056\n",
            "Epoch 2/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.3568 - accuracy: 0.3341 - ROC-AUC: 0.3982 - recall: 0.9951 - precision: 0.3220 - val_loss: 1.3264 - val_accuracy: 0.4111 - val_ROC-AUC: 0.4284 - val_recall: 0.9727 - val_precision: 0.3386\n",
            "Epoch 3/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.3105 - accuracy: 0.5878 - ROC-AUC: 0.5929 - recall: 0.8674 - precision: 0.4262 - val_loss: 1.2535 - val_accuracy: 0.6917 - val_ROC-AUC: 0.6806 - val_recall: 0.8909 - val_precision: 0.4975\n",
            "Epoch 4/50\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.2540 - accuracy: 0.6978 - ROC-AUC: 0.7092 - recall: 0.8138 - precision: 0.5148 - val_loss: 1.2160 - val_accuracy: 0.7417 - val_ROC-AUC: 0.7578 - val_recall: 0.8364 - val_precision: 0.5509\n",
            "Epoch 5/50\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.2307 - accuracy: 0.7030 - ROC-AUC: 0.7180 - recall: 0.8294 - precision: 0.5199 - val_loss: 1.2025 - val_accuracy: 0.7389 - val_ROC-AUC: 0.7531 - val_recall: 0.8364 - val_precision: 0.5476\n",
            "Epoch 6/50\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.2209 - accuracy: 0.7200 - ROC-AUC: 0.7315 - recall: 0.8226 - precision: 0.5383 - val_loss: 1.2080 - val_accuracy: 0.7556 - val_ROC-AUC: 0.7931 - val_recall: 0.7545 - val_precision: 0.5764\n",
            "Epoch 7/50\n",
            "130/130 [==============================] - 3s 21ms/step - loss: 1.2106 - accuracy: 0.7234 - ROC-AUC: 0.7425 - recall: 0.8343 - precision: 0.5414 - val_loss: 1.1926 - val_accuracy: 0.7139 - val_ROC-AUC: 0.7224 - val_recall: 0.8636 - val_precision: 0.5191\n",
            "Epoch 8/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.2056 - accuracy: 0.7302 - ROC-AUC: 0.7401 - recall: 0.8460 - precision: 0.5483 - val_loss: 1.1907 - val_accuracy: 0.7389 - val_ROC-AUC: 0.7562 - val_recall: 0.8182 - val_precision: 0.5488\n",
            "Epoch 9/50\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.1999 - accuracy: 0.7376 - ROC-AUC: 0.7536 - recall: 0.8382 - precision: 0.5574 - val_loss: 1.1899 - val_accuracy: 0.7167 - val_ROC-AUC: 0.7263 - val_recall: 0.8636 - val_precision: 0.5220\n",
            "Epoch 10/50\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.2002 - accuracy: 0.7278 - ROC-AUC: 0.7442 - recall: 0.8528 - precision: 0.5452 - val_loss: 1.1887 - val_accuracy: 0.7333 - val_ROC-AUC: 0.7642 - val_recall: 0.8182 - val_precision: 0.5422\n",
            "Epoch 11/50\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.1928 - accuracy: 0.7389 - ROC-AUC: 0.7640 - recall: 0.8528 - precision: 0.5577 - val_loss: 1.1878 - val_accuracy: 0.7250 - val_ROC-AUC: 0.7571 - val_recall: 0.8364 - val_precision: 0.5318\n",
            "Epoch 12/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1929 - accuracy: 0.7379 - ROC-AUC: 0.7634 - recall: 0.8509 - precision: 0.5568 - val_loss: 1.1864 - val_accuracy: 0.7333 - val_ROC-AUC: 0.7602 - val_recall: 0.8273 - val_precision: 0.5417\n",
            "Epoch 13/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1887 - accuracy: 0.7512 - ROC-AUC: 0.7690 - recall: 0.8489 - precision: 0.5726 - val_loss: 1.1857 - val_accuracy: 0.7389 - val_ROC-AUC: 0.7570 - val_recall: 0.8455 - val_precision: 0.5471\n",
            "Epoch 14/50\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.1905 - accuracy: 0.7373 - ROC-AUC: 0.7660 - recall: 0.8528 - precision: 0.5559 - val_loss: 1.1880 - val_accuracy: 0.7222 - val_ROC-AUC: 0.7517 - val_recall: 0.8455 - val_precision: 0.5284\n",
            "Epoch 15/50\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.1840 - accuracy: 0.7472 - ROC-AUC: 0.7727 - recall: 0.8587 - precision: 0.5669 - val_loss: 1.1969 - val_accuracy: 0.6972 - val_ROC-AUC: 0.7087 - val_recall: 0.8727 - val_precision: 0.5026\n",
            "Epoch 16/50\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.1867 - accuracy: 0.7444 - ROC-AUC: 0.7652 - recall: 0.8616 - precision: 0.5634 - val_loss: 1.2022 - val_accuracy: 0.6694 - val_ROC-AUC: 0.6883 - val_recall: 0.8909 - val_precision: 0.4780\n",
            "Epoch 17/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1840 - accuracy: 0.7460 - ROC-AUC: 0.7640 - recall: 0.8665 - precision: 0.5648 - val_loss: 1.1920 - val_accuracy: 0.7500 - val_ROC-AUC: 0.7814 - val_recall: 0.7909 - val_precision: 0.5649\n",
            "Epoch 18/50\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.1813 - accuracy: 0.7500 - ROC-AUC: 0.7616 - recall: 0.8665 - precision: 0.5695 - val_loss: 1.1886 - val_accuracy: 0.7361 - val_ROC-AUC: 0.7625 - val_recall: 0.8364 - val_precision: 0.5444\n",
            "Epoch 19/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1797 - accuracy: 0.7488 - ROC-AUC: 0.7674 - recall: 0.8733 - precision: 0.5674 - val_loss: 1.1908 - val_accuracy: 0.7056 - val_ROC-AUC: 0.7229 - val_recall: 0.8727 - val_precision: 0.5106\n",
            "Epoch 20/50\n",
            "130/130 [==============================] - 3s 20ms/step - loss: 1.1895 - accuracy: 0.7240 - ROC-AUC: 0.7427 - recall: 0.8821 - precision: 0.5397 - val_loss: 1.1861 - val_accuracy: 0.7306 - val_ROC-AUC: 0.7673 - val_recall: 0.8273 - val_precision: 0.5385\n",
            "Epoch 21/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1789 - accuracy: 0.7515 - ROC-AUC: 0.7737 - recall: 0.8684 - precision: 0.5712 - val_loss: 1.1863 - val_accuracy: 0.7278 - val_ROC-AUC: 0.7495 - val_recall: 0.8545 - val_precision: 0.5341\n",
            "Epoch 22/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1788 - accuracy: 0.7463 - ROC-AUC: 0.7605 - recall: 0.8752 - precision: 0.5644 - val_loss: 1.1917 - val_accuracy: 0.7333 - val_ROC-AUC: 0.7712 - val_recall: 0.8182 - val_precision: 0.5422\n",
            "Epoch 23/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1756 - accuracy: 0.7546 - ROC-AUC: 0.7730 - recall: 0.8655 - precision: 0.5751 - val_loss: 1.1896 - val_accuracy: 0.7333 - val_ROC-AUC: 0.7708 - val_recall: 0.8273 - val_precision: 0.5417\n",
            "Epoch 24/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1735 - accuracy: 0.7599 - ROC-AUC: 0.7765 - recall: 0.8655 - precision: 0.5815 - val_loss: 1.1889 - val_accuracy: 0.7139 - val_ROC-AUC: 0.7394 - val_recall: 0.8636 - val_precision: 0.5191\n",
            "Epoch 25/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1738 - accuracy: 0.7500 - ROC-AUC: 0.7638 - recall: 0.8801 - precision: 0.5683 - val_loss: 1.1957 - val_accuracy: 0.7278 - val_ROC-AUC: 0.7597 - val_recall: 0.8273 - val_precision: 0.5353\n",
            "Epoch 26/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1697 - accuracy: 0.7608 - ROC-AUC: 0.7739 - recall: 0.8752 - precision: 0.5816 - val_loss: 1.1862 - val_accuracy: 0.7194 - val_ROC-AUC: 0.7364 - val_recall: 0.8636 - val_precision: 0.5249\n",
            "Epoch 27/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1724 - accuracy: 0.7525 - ROC-AUC: 0.7637 - recall: 0.8821 - precision: 0.5710 - val_loss: 1.1937 - val_accuracy: 0.7222 - val_ROC-AUC: 0.7579 - val_recall: 0.8182 - val_precision: 0.5294\n",
            "Epoch 28/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1663 - accuracy: 0.7664 - ROC-AUC: 0.7780 - recall: 0.8743 - precision: 0.5886 - val_loss: 1.1877 - val_accuracy: 0.7250 - val_ROC-AUC: 0.7363 - val_recall: 0.8545 - val_precision: 0.5311\n",
            "Epoch 29/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1658 - accuracy: 0.7679 - ROC-AUC: 0.7806 - recall: 0.8733 - precision: 0.5906 - val_loss: 1.1927 - val_accuracy: 0.7333 - val_ROC-AUC: 0.7621 - val_recall: 0.8273 - val_precision: 0.5417\n",
            "Epoch 30/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1671 - accuracy: 0.7642 - ROC-AUC: 0.7761 - recall: 0.8791 - precision: 0.5853 - val_loss: 1.1983 - val_accuracy: 0.7556 - val_ROC-AUC: 0.7805 - val_recall: 0.7818 - val_precision: 0.5733\n",
            "Epoch 31/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1683 - accuracy: 0.7627 - ROC-AUC: 0.7735 - recall: 0.8752 - precision: 0.5839 - val_loss: 1.1934 - val_accuracy: 0.7278 - val_ROC-AUC: 0.7706 - val_recall: 0.8091 - val_precision: 0.5361\n",
            "Epoch 32/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1638 - accuracy: 0.7679 - ROC-AUC: 0.7789 - recall: 0.8752 - precision: 0.5904 - val_loss: 1.1903 - val_accuracy: 0.7306 - val_ROC-AUC: 0.7485 - val_recall: 0.8455 - val_precision: 0.5376\n",
            "Epoch 33/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1675 - accuracy: 0.7704 - ROC-AUC: 0.7840 - recall: 0.8616 - precision: 0.5953 - val_loss: 1.1983 - val_accuracy: 0.7278 - val_ROC-AUC: 0.7619 - val_recall: 0.8091 - val_precision: 0.5361\n",
            "Epoch 34/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1618 - accuracy: 0.7633 - ROC-AUC: 0.7708 - recall: 0.8957 - precision: 0.5824 - val_loss: 1.1882 - val_accuracy: 0.7306 - val_ROC-AUC: 0.7432 - val_recall: 0.8455 - val_precision: 0.5376\n",
            "Epoch 35/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1611 - accuracy: 0.7701 - ROC-AUC: 0.7766 - recall: 0.8840 - precision: 0.5920 - val_loss: 1.1933 - val_accuracy: 0.7333 - val_ROC-AUC: 0.7561 - val_recall: 0.8273 - val_precision: 0.5417\n",
            "Epoch 36/50\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.1622 - accuracy: 0.7698 - ROC-AUC: 0.7805 - recall: 0.8801 - precision: 0.5921 - val_loss: 1.1936 - val_accuracy: 0.7278 - val_ROC-AUC: 0.7521 - val_recall: 0.8273 - val_precision: 0.5353\n",
            "Epoch 37/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1627 - accuracy: 0.7636 - ROC-AUC: 0.7732 - recall: 0.8850 - precision: 0.5839 - val_loss: 1.1937 - val_accuracy: 0.7306 - val_ROC-AUC: 0.7665 - val_recall: 0.8182 - val_precision: 0.5389\n",
            "Epoch 38/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1575 - accuracy: 0.7738 - ROC-AUC: 0.7814 - recall: 0.8908 - precision: 0.5958 - val_loss: 1.1912 - val_accuracy: 0.7167 - val_ROC-AUC: 0.7330 - val_recall: 0.8545 - val_precision: 0.5222\n",
            "Epoch 39/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1578 - accuracy: 0.7710 - ROC-AUC: 0.7788 - recall: 0.8899 - precision: 0.5925 - val_loss: 1.1874 - val_accuracy: 0.7306 - val_ROC-AUC: 0.7363 - val_recall: 0.8545 - val_precision: 0.5371\n",
            "Epoch 40/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1572 - accuracy: 0.7735 - ROC-AUC: 0.7806 - recall: 0.8918 - precision: 0.5953 - val_loss: 1.1901 - val_accuracy: 0.7333 - val_ROC-AUC: 0.7549 - val_recall: 0.8273 - val_precision: 0.5417\n",
            "Epoch 41/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1563 - accuracy: 0.7760 - ROC-AUC: 0.7808 - recall: 0.8918 - precision: 0.5984 - val_loss: 1.1953 - val_accuracy: 0.7278 - val_ROC-AUC: 0.7609 - val_recall: 0.8182 - val_precision: 0.5357\n",
            "Epoch 42/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1581 - accuracy: 0.7698 - ROC-AUC: 0.7762 - recall: 0.8918 - precision: 0.5907 - val_loss: 1.1901 - val_accuracy: 0.7222 - val_ROC-AUC: 0.7370 - val_recall: 0.8545 - val_precision: 0.5281\n",
            "Epoch 43/50\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.1546 - accuracy: 0.7769 - ROC-AUC: 0.7853 - recall: 0.8869 - precision: 0.6003 - val_loss: 1.1989 - val_accuracy: 0.7306 - val_ROC-AUC: 0.7613 - val_recall: 0.8182 - val_precision: 0.5389\n",
            "Epoch 44/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1539 - accuracy: 0.7775 - ROC-AUC: 0.7817 - recall: 0.8889 - precision: 0.6008 - val_loss: 1.1991 - val_accuracy: 0.7417 - val_ROC-AUC: 0.7698 - val_recall: 0.7909 - val_precision: 0.5541\n",
            "Epoch 45/50\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.1525 - accuracy: 0.7778 - ROC-AUC: 0.7817 - recall: 0.8899 - precision: 0.6011 - val_loss: 1.1930 - val_accuracy: 0.7306 - val_ROC-AUC: 0.7516 - val_recall: 0.8273 - val_precision: 0.5385\n",
            "Epoch 46/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1559 - accuracy: 0.7741 - ROC-AUC: 0.7824 - recall: 0.8869 - precision: 0.5967 - val_loss: 1.1952 - val_accuracy: 0.7222 - val_ROC-AUC: 0.7409 - val_recall: 0.8364 - val_precision: 0.5287\n",
            "Epoch 47/50\n",
            "130/130 [==============================] - 2s 18ms/step - loss: 1.1612 - accuracy: 0.7794 - ROC-AUC: 0.7880 - recall: 0.8674 - precision: 0.6063 - val_loss: 1.1887 - val_accuracy: 0.7306 - val_ROC-AUC: 0.7466 - val_recall: 0.8364 - val_precision: 0.5380\n",
            "Epoch 48/50\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.1505 - accuracy: 0.7828 - ROC-AUC: 0.7856 - recall: 0.8918 - precision: 0.6072 - val_loss: 1.1904 - val_accuracy: 0.7250 - val_ROC-AUC: 0.7454 - val_recall: 0.8364 - val_precision: 0.5318\n",
            "Epoch 49/50\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.1716 - accuracy: 0.7676 - ROC-AUC: 0.7872 - recall: 0.8567 - precision: 0.5923 - val_loss: 1.1981 - val_accuracy: 0.7139 - val_ROC-AUC: 0.7468 - val_recall: 0.8455 - val_precision: 0.5196\n",
            "Epoch 50/50\n",
            "130/130 [==============================] - 2s 17ms/step - loss: 1.1573 - accuracy: 0.7763 - ROC-AUC: 0.7922 - recall: 0.8821 - precision: 0.6001 - val_loss: 1.2028 - val_accuracy: 0.7417 - val_ROC-AUC: 0.7615 - val_recall: 0.7909 - val_precision: 0.5541\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    workers=4,\n",
        "    use_multiprocessing=True,\n",
        "    callbacks=[metric_logger,]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9rAVR2c08hd"
      },
      "source": [
        "# Terminate WandB Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "L4RoKqcV3DKm",
        "outputId": "12403bfa-5c9d-4583-b222-80e84df772b8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/ROC-AUC</td><td>▁▂▅▇▇▇▇▇█████████▇██████████████████████</td></tr><tr><td>epoch/accuracy</td><td>▁▁▅▇▇▇▇▇▇▇█▇▇▇▇▇█▇██████████████████████</td></tr><tr><td>epoch/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>█▇▆▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/precision</td><td>▁▁▄▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇██▇█▇██████████</td></tr><tr><td>epoch/recall</td><td>▅█▃▁▁▂▂▂▃▂▂▃▃▃▃▃▃▃▃▃▃▄▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▃▄▄</td></tr><tr><td>epoch/val_ROC-AUC</td><td>▁▂▆▇█▇▇▇▇▇▇▇▆█▇▇▇██▇▇▇▇▇█▇▇▇▇█▇▇▇▇▇█▇▇▇▇</td></tr><tr><td>epoch/val_accuracy</td><td>▁▃▇██▇█▇███▇▇██▇███▇▇▇████████▇██▇██▇███</td></tr><tr><td>epoch/val_loss</td><td>█▇▄▂▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂▂▁▁▁▂</td></tr><tr><td>epoch/val_precision</td><td>▁▂▆▇█▇▇▇▇▇▇▇▅█▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>epoch/val_recall</td><td>█▇▅▃▁▄▃▄▃▃▄▄▅▂▃▄▄▃▃▄▄▃▄▃▃▄▃▄▃▃▄▄▃▄▃▂▃▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/ROC-AUC</td><td>0.79221</td></tr><tr><td>epoch/accuracy</td><td>0.77627</td></tr><tr><td>epoch/epoch</td><td>49</td></tr><tr><td>epoch/learning_rate</td><td>3e-05</td></tr><tr><td>epoch/loss</td><td>1.15731</td></tr><tr><td>epoch/precision</td><td>0.60013</td></tr><tr><td>epoch/recall</td><td>0.88207</td></tr><tr><td>epoch/val_ROC-AUC</td><td>0.76147</td></tr><tr><td>epoch/val_accuracy</td><td>0.74167</td></tr><tr><td>epoch/val_loss</td><td>1.20279</td></tr><tr><td>epoch/val_precision</td><td>0.55414</td></tr><tr><td>epoch/val_recall</td><td>0.79091</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">bi-LSTM LR:3e-05 B:25 W:5.0 E:100</strong> at: <a href=\"https://wandb.ai/samousavizade/CoffeeBazaarSeqClassification/runs/dg1rnufx\" target=\"_blank\">https://wandb.ai/samousavizade/CoffeeBazaarSeqClassification/runs/dg1rnufx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230204_191749-dg1rnufx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2lLaKvMyJzh"
      },
      "source": [
        "# Test Phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lWrcg4OyLH6",
        "outputId": "05ebcddd-bbc9-4ac1-c913-8e636077dba5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Case Text:\n",
            "!!!!سلام برنامه خوبیه جدا\n",
            "Prediction:\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "[[0.7829611  0.21703894]]\n",
            "********************************************************************************\n",
            "Test Case Text:\n",
            "لود نمیشه اصلا!! :((((\n",
            "Prediction:\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "[[0.2591255 0.7408745]]\n",
            "********************************************************************************\n",
            "Test Case Text:\n",
            "پولم رو پس نمیدید چرا؟؟؟\n",
            "Prediction:\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "[[0.4458921  0.55410796]]\n",
            "********************************************************************************\n",
            "Test Case Text:\n",
            "بازی جالبیه.\n",
            "Prediction:\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[[0.71512353 0.28487647]]\n",
            "********************************************************************************\n",
            "Test Case Text:\n",
            "خیلییییی لگ داره روی گوشیم.\n",
            "Prediction:\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "[[0.8958438  0.10415614]]\n",
            "********************************************************************************\n",
            "Test Case Text:\n",
            "معتاد این بازی شدم.\n",
            "Prediction:\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[[0.7261366  0.27386335]]\n",
            "********************************************************************************\n",
            "Test Case Text:\n",
            "خیلی باگ داره اعصابو خورد کرده.\n",
            "Prediction:\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "[[0.40156168 0.5984383 ]]\n",
            "********************************************************************************\n",
            "Test Case Text:\n",
            "بازی توی مرحله اول گیر کرده و به مرحله بعدی نمیره اصلا! :(((\n",
            "Prediction:\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "[[0.01163175 0.98836833]]\n",
            "********************************************************************************\n",
            "Test Case Text:\n",
            "آقا عالیه!!!!\n",
            "Prediction:\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[[0.674943   0.32505706]]\n",
            "********************************************************************************\n",
            "Test Case Text:\n",
            "موقع نصب به مشکل میخوره. اه.\n",
            "Prediction:\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[[0.2205613  0.77943873]]\n",
            "********************************************************************************\n",
            "Test Case Text:\n",
            "آشغااااااااااااااله\n",
            "Prediction:\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[[0.5383584 0.4616416]]\n",
            "********************************************************************************\n",
            "Test Case Text:\n",
            "افتضاحهههههه.\n",
            "Prediction:\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "[[0.5286947  0.47130537]]\n",
            "********************************************************************************\n",
            "Test Case Text:\n",
            "مزخرفه.\n",
            "Prediction:\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "[[0.5272485  0.47275147]]\n",
            "********************************************************************************\n",
            "Test Case Text:\n",
            "همش باگ میخورههههههههههههههههههههه.\n",
            "Prediction:\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "[[0.5216382 0.4783618]]\n",
            "********************************************************************************\n",
            "Test Case Text:\n",
            "برای بچه ها مشکل داره این بازی. لطفا اینو ذکر کنید.\n",
            "Prediction:\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "[[0.8227697  0.17723022]]\n",
            "********************************************************************************\n"
          ]
        }
      ],
      "source": [
        "for i in range(test_case_data.shape[0]):\n",
        "  test_case_text = test_case_data[\"text\"][i]\n",
        "  preprocessed_test_case = test_case_data[\"lemmatized_tokens\"][i]\n",
        "  \n",
        "  print(\"Test Case Text:\")\n",
        "  print(test_case_text)\n",
        "  print(\"Prediction:\")\n",
        "  print(model.predict([preprocessed_test_case, ]))\n",
        "  print(\"*\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ekFPcS7FcYvc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2ff50100434cc479892626f133397731c49cdeb5620cb5b4c3a938fe7a88ca82"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
